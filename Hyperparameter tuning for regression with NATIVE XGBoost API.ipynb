{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=black><br>\n",
    "\n",
    "**What?** Hyperparameter tuning for regression with NATIVE XGBoost API\n",
    "\n",
    "**Reference:** https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f<br>\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why would you use the native XGBoost API over the scikit-learn API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "Advantages include:\n",
    "1. Automatically find the best number of boosting rounds\n",
    "- Built-in cross validation\n",
    "- Custom objective functions\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Facebook comment volume dataset\n",
    "- Dataset can be donwload here: https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset\n",
    "- 53 features describing a Facebook post: the number of likes on the page it was posted, the category of the page, the time and day it was posted, etc. \n",
    "- The last column is the target: the number of comments the post received. \n",
    "- **GOAL**: predict the number of comments a new post will receive based on all the given features.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features_Variant_1.arff Features_Variant_3.arff Features_Variant_5.arff\r\n",
      "Features_Variant_1.csv  Features_Variant_3.csv  Features_Variant_5.csv\r\n",
      "Features_Variant_2.arff Features_Variant_4.arff\r\n",
      "Features_Variant_2.csv  Features_Variant_4.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../DATASETS/Facebook_comment_volume_dataset/Training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22151</th>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>5365996</td>\n",
       "      <td>40729</td>\n",
       "      <td>102442</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>60.612150</td>\n",
       "      <td>33.0</td>\n",
       "      <td>90.050007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29452</th>\n",
       "      <td>204478</td>\n",
       "      <td>0</td>\n",
       "      <td>3661</td>\n",
       "      <td>92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>51.688525</td>\n",
       "      <td>13.0</td>\n",
       "      <td>79.076543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7224</th>\n",
       "      <td>441897</td>\n",
       "      <td>0</td>\n",
       "      <td>16175</td>\n",
       "      <td>18</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>99.691275</td>\n",
       "      <td>71.0</td>\n",
       "      <td>118.840986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15889</th>\n",
       "      <td>7564986</td>\n",
       "      <td>0</td>\n",
       "      <td>123241</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.365714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.335913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1       2   3    4       5          6     7           8   \\\n",
       "22151      497      0       0  16  0.0     5.0   0.381818   0.0    0.884018   \n",
       "27582  5365996  40729  102442   9  0.0   740.0  60.612150  33.0   90.050007   \n",
       "29452   204478      0    3661  92  0.0   368.0  51.688525  13.0   79.076543   \n",
       "7224    441897      0   16175  18  8.0  1164.0  99.691275  71.0  118.840986   \n",
       "15889  7564986      0  123241  12  0.0    35.0   3.365714   1.0    5.335913   \n",
       "\n",
       "        9   ...  44  45  46  47  48  49  50  51  52  53  \n",
       "22151  0.0  ...   0   0   1   0   0   0   0   0   0   0  \n",
       "27582  0.0  ...   0   0   0   0   0   1   0   0   0   3  \n",
       "29452  0.0  ...   0   0   1   0   0   0   0   0   0   7  \n",
       "7224   0.0  ...   0   0   0   0   0   0   0   1   0   0  \n",
       "15889  0.0  ...   0   0   0   0   0   1   0   0   0   0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"../DATASETS/Facebook_comment_volume_dataset/Training/Features_Variant_1.csv\"\n",
    "df = pd.read_csv(file, header=None)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 40949 entries and 54 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset has {} entries and {} features\".format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- In order to use the native API for XGBoost, we will first need to build DMatrices.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:,:52].values, df.loc[:,53].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- The **baseline model** serves to get a score which can be achieved with no efforts.\n",
    "- The hope is to beat it with our tuned/fancy algorithm.\n",
    "- The **MAE** Mean Absolute Error is here chosen becasuse it has the same unit of the target and that ease the results interpretation.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean form the training data 7.28\n",
      "Mean form the test data 7.72\n",
      "Baseline MAE is 11.31\n"
     ]
    }
   ],
   "source": [
    "# \"Learn\" the mean from the training data\n",
    "mean_train = np.mean(y_train)\n",
    "mean_test = np.mean(y_test)\n",
    "print(\"Mean form the training data {:.2f}\".format(mean_train))\n",
    "print(\"Mean form the test data {:.2f}\".format(mean_test))\n",
    "\n",
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE is {:.2f}\".format(mae_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- **Is our baseline model good**? \n",
    "- Our prediction is 11.31 comments away from the truth.\n",
    "- That is not good if we compare it against the average number for a post in both training and test set.\n",
    "- Of course is the error the other is mean, so they are not exactly the same thing, but we can still use it to get the order of magnitude.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How num_boost_round & early_stopping_rounds are used in tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- There are 2 other parameters that are passed as a standalone argument of XGBoost that are not the params dictionary.\n",
    "- The num_boost_round and corresponds to the No of boosting rounds or trees to build. \n",
    "- You could tune it together with all parameters in a grid-search, but it’ll be **expensive**.\n",
    "- There is a **more efficient** way. Since trees are built sequentially, instead of fixing the number of rounds at the beginning, we can test our model at each step and see if adding a new tree/round improves performance.\n",
    "- To do so, we define a test dataset and a metric that is used to assess performance at each round. If performance haven’t improved for N rounds (N is defined by the variable early_stopping_round), we stop the training and keep the best number of boosting rounds.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eval_metric'] = \"mae\"\n",
    "num_boost_round = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:10:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tTest-mae:5.97481\n",
      "Will train until Test-mae hasn't improved in 10 rounds.\n",
      "[1]\tTest-mae:5.03353\n",
      "[2]\tTest-mae:4.64575\n",
      "[3]\tTest-mae:4.42335\n",
      "[4]\tTest-mae:4.39328\n",
      "[5]\tTest-mae:4.35536\n",
      "[6]\tTest-mae:4.31313\n",
      "[7]\tTest-mae:4.33087\n",
      "[8]\tTest-mae:4.37167\n",
      "[9]\tTest-mae:4.38777\n",
      "[10]\tTest-mae:4.39438\n",
      "[11]\tTest-mae:4.40656\n",
      "[12]\tTest-mae:4.39122\n",
      "[13]\tTest-mae:4.39086\n",
      "[14]\tTest-mae:4.39829\n",
      "[15]\tTest-mae:4.39103\n",
      "[16]\tTest-mae:4.40305\n",
      "Stopping. Best iteration:\n",
      "[6]\tTest-mae:4.31313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round = num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- As you can see we stopped before reaching the maximum number of boosting rounds, that’s because after the 7th tree, adding more rounds did not lead to improvements of MAE on the test dataset.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: 4.31 with 7 rounds\n"
     ]
    }
   ],
   "source": [
    "print(\"Best MAE: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use XGBoost native CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- We **don’t need** to pass a test dataset here. \n",
    "- It’s because the cross-validation function is splitting the train dataset into nfolds and iteratively keeps one of the folds for test purposes. \n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:10:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:10:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:10:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:10:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:10:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'mae'},\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- cv returns a table where the rows correspond to the **No of boosting** trees used.\n",
    "- What is important to note is that we stopped before the 999 rounds (fortunately!).\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-mae-mean</th>\n",
       "      <th>train-mae-std</th>\n",
       "      <th>test-mae-mean</th>\n",
       "      <th>test-mae-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.604948</td>\n",
       "      <td>0.064660</td>\n",
       "      <td>5.689212</td>\n",
       "      <td>0.270147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.622352</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>4.849511</td>\n",
       "      <td>0.271889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.059494</td>\n",
       "      <td>0.065932</td>\n",
       "      <td>4.468344</td>\n",
       "      <td>0.239464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.723084</td>\n",
       "      <td>0.060754</td>\n",
       "      <td>4.268582</td>\n",
       "      <td>0.224425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.510358</td>\n",
       "      <td>0.061148</td>\n",
       "      <td>4.192462</td>\n",
       "      <td>0.189760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.367076</td>\n",
       "      <td>0.060926</td>\n",
       "      <td>4.172847</td>\n",
       "      <td>0.189624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.245542</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>4.157830</td>\n",
       "      <td>0.192568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.151558</td>\n",
       "      <td>0.062634</td>\n",
       "      <td>4.143255</td>\n",
       "      <td>0.194406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.082316</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>4.147838</td>\n",
       "      <td>0.196198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.016860</td>\n",
       "      <td>0.057426</td>\n",
       "      <td>4.144695</td>\n",
       "      <td>0.189789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.974902</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>4.151545</td>\n",
       "      <td>0.184565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.929200</td>\n",
       "      <td>0.034232</td>\n",
       "      <td>4.153972</td>\n",
       "      <td>0.192398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.900069</td>\n",
       "      <td>0.037116</td>\n",
       "      <td>4.154957</td>\n",
       "      <td>0.192696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.870256</td>\n",
       "      <td>0.039896</td>\n",
       "      <td>4.150084</td>\n",
       "      <td>0.185111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.846606</td>\n",
       "      <td>0.037852</td>\n",
       "      <td>4.156712</td>\n",
       "      <td>0.188309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.811940</td>\n",
       "      <td>0.036952</td>\n",
       "      <td>4.145419</td>\n",
       "      <td>0.184141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.791494</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>4.141663</td>\n",
       "      <td>0.183337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.764766</td>\n",
       "      <td>0.032827</td>\n",
       "      <td>4.139395</td>\n",
       "      <td>0.191487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.730392</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>4.134148</td>\n",
       "      <td>0.196816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.686273</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>4.120617</td>\n",
       "      <td>0.194068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.661705</td>\n",
       "      <td>0.024592</td>\n",
       "      <td>4.115607</td>\n",
       "      <td>0.190027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.632633</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>4.111506</td>\n",
       "      <td>0.195612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.598405</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>4.106893</td>\n",
       "      <td>0.188209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.575784</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>4.109432</td>\n",
       "      <td>0.189704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.544757</td>\n",
       "      <td>0.030729</td>\n",
       "      <td>4.106909</td>\n",
       "      <td>0.185136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.525584</td>\n",
       "      <td>0.031770</td>\n",
       "      <td>4.103466</td>\n",
       "      <td>0.185227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.500868</td>\n",
       "      <td>0.027647</td>\n",
       "      <td>4.098627</td>\n",
       "      <td>0.187003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.478802</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>4.098479</td>\n",
       "      <td>0.185771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.457501</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>4.098723</td>\n",
       "      <td>0.186916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.433874</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>4.093531</td>\n",
       "      <td>0.187030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.413470</td>\n",
       "      <td>0.019401</td>\n",
       "      <td>4.092384</td>\n",
       "      <td>0.190348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.398091</td>\n",
       "      <td>0.020366</td>\n",
       "      <td>4.092821</td>\n",
       "      <td>0.189489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.373672</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>4.094028</td>\n",
       "      <td>0.191975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.354035</td>\n",
       "      <td>0.021117</td>\n",
       "      <td>4.090820</td>\n",
       "      <td>0.183615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.341472</td>\n",
       "      <td>0.014112</td>\n",
       "      <td>4.091563</td>\n",
       "      <td>0.183086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.325646</td>\n",
       "      <td>0.011124</td>\n",
       "      <td>4.092404</td>\n",
       "      <td>0.179458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.307252</td>\n",
       "      <td>0.014042</td>\n",
       "      <td>4.086721</td>\n",
       "      <td>0.183273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.292454</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>4.082794</td>\n",
       "      <td>0.184471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
       "0         5.604948       0.064660       5.689212      0.270147\n",
       "1         4.622352       0.065104       4.849511      0.271889\n",
       "2         4.059494       0.065932       4.468344      0.239464\n",
       "3         3.723084       0.060754       4.268582      0.224425\n",
       "4         3.510358       0.061148       4.192462      0.189760\n",
       "5         3.367076       0.060926       4.172847      0.189624\n",
       "6         3.245542       0.060118       4.157830      0.192568\n",
       "7         3.151558       0.062634       4.143255      0.194406\n",
       "8         3.082316       0.058967       4.147838      0.196198\n",
       "9         3.016860       0.057426       4.144695      0.189789\n",
       "10        2.974902       0.048559       4.151545      0.184565\n",
       "11        2.929200       0.034232       4.153972      0.192398\n",
       "12        2.900069       0.037116       4.154957      0.192696\n",
       "13        2.870256       0.039896       4.150084      0.185111\n",
       "14        2.846606       0.037852       4.156712      0.188309\n",
       "15        2.811940       0.036952       4.145419      0.184141\n",
       "16        2.791494       0.034450       4.141663      0.183337\n",
       "17        2.764766       0.032827       4.139395      0.191487\n",
       "18        2.730392       0.030303       4.134148      0.196816\n",
       "19        2.686273       0.025707       4.120617      0.194068\n",
       "20        2.661705       0.024592       4.115607      0.190027\n",
       "21        2.632633       0.026915       4.111506      0.195612\n",
       "22        2.598405       0.026599       4.106893      0.188209\n",
       "23        2.575784       0.034963       4.109432      0.189704\n",
       "24        2.544757       0.030729       4.106909      0.185136\n",
       "25        2.525584       0.031770       4.103466      0.185227\n",
       "26        2.500868       0.027647       4.098627      0.187003\n",
       "27        2.478802       0.029684       4.098479      0.185771\n",
       "28        2.457501       0.027980       4.098723      0.186916\n",
       "29        2.433874       0.019580       4.093531      0.187030\n",
       "30        2.413470       0.019401       4.092384      0.190348\n",
       "31        2.398091       0.020366       4.092821      0.189489\n",
       "32        2.373672       0.023378       4.094028      0.191975\n",
       "33        2.354035       0.021117       4.090820      0.183615\n",
       "34        2.341472       0.014112       4.091563      0.183086\n",
       "35        2.325646       0.011124       4.092404      0.179458\n",
       "36        2.307252       0.014042       4.086721      0.183273\n",
       "37        2.292454       0.011215       4.082794      0.184471"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0827944"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test-mae-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Tuning - bringing everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-related hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- These 2 parameters can be used to control the complexity of the trees.\n",
    "- **max_depth** is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships by adding more nodes, but as we go deeper, splits become less relevant and are sometimes only due to noise, causing the model to overfit.\n",
    "- **min_child_weight** is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n",
    "- It is important to tune them together in order to find a good **trade-off** between model bias and variance\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of tuples\n",
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n",
      "[13:11:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.045249799999999 for 6 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "[13:11:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.07648 for 5 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "[13:11:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0753965999999995 for 5 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "[13:11:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:11:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0805982 for 5 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "[13:12:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0351186 for 5 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "[13:12:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0872286 for 5 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "[13:12:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0626337999999995 for 5 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "[13:12:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:12:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.054813 for 5 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "[13:13:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0580998 for 5 rounds\n"
     ]
    }
   ],
   "source": [
    "# Define initial best params and MAE\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: 10, 6, MAE: 4.0351186\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update the parameters dictionary\n",
    "params['max_depth'] = 10\n",
    "params['min_child_weight'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.3,\n",
       " 'subsample': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'objective': 'reg:linear',\n",
       " 'eval_metric': 'mae'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Those parameters control the sampling of the dataset that is done at each boosting round.\n",
    "- Instead of using the whole training set every time, we can build a tree on slightly different data at each step, which makes it less likely to overfit to a single sample or feature.\n",
    "- **subsample** corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n",
    "- **colsample_bytree** corresponds to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7, 0.7),\n",
       " (0.7, 0.8),\n",
       " (0.7, 0.9),\n",
       " (0.7, 1.0),\n",
       " (0.8, 0.7),\n",
       " (0.8, 0.8),\n",
       " (0.8, 0.9),\n",
       " (0.8, 1.0),\n",
       " (0.9, 0.7),\n",
       " (0.9, 0.8),\n",
       " (0.9, 0.9),\n",
       " (0.9, 1.0),\n",
       " (1.0, 0.7),\n",
       " (1.0, 0.8),\n",
       " (1.0, 0.9),\n",
       " (1.0, 1.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "[13:13:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:23] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0351186 for 5 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "[13:13:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.086857 for 5 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "[13:13:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:13:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.1143116 for 5 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "[13:14:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.2003104 for 5 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "[13:14:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0303374000000005 for 5 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "[13:14:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0712008 for 5 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "[13:14:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.1030088 for 4 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "[13:14:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:14:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.1687326 for 4 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "[13:15:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.149029999999999 for 5 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "[13:15:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.157543800000001 for 6 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "[13:15:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.1861581999999995 for 7 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "[13:15:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:15:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.1982572 for 4 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "[13:16:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0902796 for 5 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "[13:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:19] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:19] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:19] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.112025999999999 for 6 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "[13:16:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.1948692 for 4 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "[13:16:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.19326 for 4 rounds\n"
     ]
    }
   ],
   "source": [
    "min_mae = float(\"Inf\")\n",
    "best_params = None# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: 0.9, 1.0, MAE: 4.0303374000000005\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update the params dict\n",
    "params['subsample'] = .8\n",
    "params['colsample_bytree'] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.3,\n",
       " 'subsample': 0.8,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'objective': 'reg:linear',\n",
       " 'eval_metric': 'mae'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter ETA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- The ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step.\n",
    "- In practice, having a lower eta makes our model more robust to overfitting thus, usually, the lower the learning rate, the best. \n",
    "- But with a lower eta, we need more boosting rounds, which takes more time to train, sometimes for only marginal improvements.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "[13:16:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:16:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.149029999999999 for 5 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "[13:17:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 4.0048536 for 10 rounds\n",
      "\n",
      "CV with eta=0.1\n",
      "[13:17:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:17:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 3.9243398 for 19 rounds\n",
      "\n",
      "CV with eta=0.05\n",
      "[13:18:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:18:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:18:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:18:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:18:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 3.8693966000000004 for 46 rounds\n",
      "\n",
      "CV with eta=0.01\n",
      "[13:19:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:19:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:19:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:19:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:19:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 3.8336379999999997 for 235 rounds\n",
      "\n",
      "CV with eta=0.005\n",
      "[13:23:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:23:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:23:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:23:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:23:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\tMAE 3.8281038 for 479 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This can take some time…\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))    \n",
    "    \n",
    "    # We update our parameters\n",
    "    params['eta'] = eta    \n",
    "    \n",
    "    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics=['mae'],\n",
    "    early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: 0.005, MAE: 3.8281038\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update the params dict\n",
    "params['ETA'] = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.005,\n",
       " 'subsample': 0.8,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'objective': 'reg:linear',\n",
       " 'eval_metric': 'mae',\n",
       " 'ETA': 0.005}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and get test set results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Please note that the best No of boosting round does not follow a monotonic trend.\n",
    "- This means the best value is not the last one!\n",
    "- This important when we want to save the model. \n",
    "- In fact if we know the best number of boosting we do not need to use the **ealy_stopping_round** anymore.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:32] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tTest-mae:7.72516\n",
      "Will train until Test-mae hasn't improved in 10 rounds.\n",
      "[1]\tTest-mae:7.68929\n",
      "[2]\tTest-mae:7.65557\n",
      "[3]\tTest-mae:7.61957\n",
      "[4]\tTest-mae:7.5839\n",
      "[5]\tTest-mae:7.54856\n",
      "[6]\tTest-mae:7.51315\n",
      "[7]\tTest-mae:7.48022\n",
      "[8]\tTest-mae:7.44596\n",
      "[9]\tTest-mae:7.41321\n",
      "[10]\tTest-mae:7.37869\n",
      "[11]\tTest-mae:7.34604\n",
      "[12]\tTest-mae:7.31248\n",
      "[13]\tTest-mae:7.2791\n",
      "[14]\tTest-mae:7.24591\n",
      "[15]\tTest-mae:7.21429\n",
      "[16]\tTest-mae:7.18185\n",
      "[17]\tTest-mae:7.14933\n",
      "[18]\tTest-mae:7.11726\n",
      "[19]\tTest-mae:7.08604\n",
      "[20]\tTest-mae:7.05399\n",
      "[21]\tTest-mae:7.02381\n",
      "[22]\tTest-mae:6.99194\n",
      "[23]\tTest-mae:6.96059\n",
      "[24]\tTest-mae:6.92885\n",
      "[25]\tTest-mae:6.89883\n",
      "[26]\tTest-mae:6.86869\n",
      "[27]\tTest-mae:6.84059\n",
      "[28]\tTest-mae:6.81215\n",
      "[29]\tTest-mae:6.78097\n",
      "[30]\tTest-mae:6.75338\n",
      "[31]\tTest-mae:6.72323\n",
      "[32]\tTest-mae:6.69574\n",
      "[33]\tTest-mae:6.66586\n",
      "[34]\tTest-mae:6.63746\n",
      "[35]\tTest-mae:6.60968\n",
      "[36]\tTest-mae:6.58174\n",
      "[37]\tTest-mae:6.55553\n",
      "[38]\tTest-mae:6.52827\n",
      "[39]\tTest-mae:6.50107\n",
      "[40]\tTest-mae:6.47334\n",
      "[41]\tTest-mae:6.44917\n",
      "[42]\tTest-mae:6.42616\n",
      "[43]\tTest-mae:6.39965\n",
      "[44]\tTest-mae:6.37425\n",
      "[45]\tTest-mae:6.34918\n",
      "[46]\tTest-mae:6.32219\n",
      "[47]\tTest-mae:6.29722\n",
      "[48]\tTest-mae:6.27235\n",
      "[49]\tTest-mae:6.24652\n",
      "[50]\tTest-mae:6.22324\n",
      "[51]\tTest-mae:6.19966\n",
      "[52]\tTest-mae:6.17828\n",
      "[53]\tTest-mae:6.15846\n",
      "[54]\tTest-mae:6.13553\n",
      "[55]\tTest-mae:6.11277\n",
      "[56]\tTest-mae:6.09179\n",
      "[57]\tTest-mae:6.06978\n",
      "[58]\tTest-mae:6.04734\n",
      "[59]\tTest-mae:6.02683\n",
      "[60]\tTest-mae:6.00448\n",
      "[61]\tTest-mae:5.98486\n",
      "[62]\tTest-mae:5.96288\n",
      "[63]\tTest-mae:5.9428\n",
      "[64]\tTest-mae:5.92207\n",
      "[65]\tTest-mae:5.90204\n",
      "[66]\tTest-mae:5.88134\n",
      "[67]\tTest-mae:5.86229\n",
      "[68]\tTest-mae:5.84278\n",
      "[69]\tTest-mae:5.82292\n",
      "[70]\tTest-mae:5.80246\n",
      "[71]\tTest-mae:5.78308\n",
      "[72]\tTest-mae:5.76214\n",
      "[73]\tTest-mae:5.74275\n",
      "[74]\tTest-mae:5.72307\n",
      "[75]\tTest-mae:5.70526\n",
      "[76]\tTest-mae:5.68738\n",
      "[77]\tTest-mae:5.66891\n",
      "[78]\tTest-mae:5.64965\n",
      "[79]\tTest-mae:5.63011\n",
      "[80]\tTest-mae:5.61244\n",
      "[81]\tTest-mae:5.59495\n",
      "[82]\tTest-mae:5.57802\n",
      "[83]\tTest-mae:5.55997\n",
      "[84]\tTest-mae:5.54248\n",
      "[85]\tTest-mae:5.52402\n",
      "[86]\tTest-mae:5.50544\n",
      "[87]\tTest-mae:5.48793\n",
      "[88]\tTest-mae:5.47144\n",
      "[89]\tTest-mae:5.45407\n",
      "[90]\tTest-mae:5.43767\n",
      "[91]\tTest-mae:5.42289\n",
      "[92]\tTest-mae:5.40552\n",
      "[93]\tTest-mae:5.38992\n",
      "[94]\tTest-mae:5.37418\n",
      "[95]\tTest-mae:5.35804\n",
      "[96]\tTest-mae:5.34324\n",
      "[97]\tTest-mae:5.32699\n",
      "[98]\tTest-mae:5.31168\n",
      "[99]\tTest-mae:5.29593\n",
      "[100]\tTest-mae:5.28041\n",
      "[101]\tTest-mae:5.26556\n",
      "[102]\tTest-mae:5.25007\n",
      "[103]\tTest-mae:5.23694\n",
      "[104]\tTest-mae:5.22247\n",
      "[105]\tTest-mae:5.20872\n",
      "[106]\tTest-mae:5.19456\n",
      "[107]\tTest-mae:5.17902\n",
      "[108]\tTest-mae:5.16427\n",
      "[109]\tTest-mae:5.14971\n",
      "[110]\tTest-mae:5.13717\n",
      "[111]\tTest-mae:5.12298\n",
      "[112]\tTest-mae:5.1099\n",
      "[113]\tTest-mae:5.09539\n",
      "[114]\tTest-mae:5.08056\n",
      "[115]\tTest-mae:5.0666\n",
      "[116]\tTest-mae:5.05295\n",
      "[117]\tTest-mae:5.03837\n",
      "[118]\tTest-mae:5.02372\n",
      "[119]\tTest-mae:5.00985\n",
      "[120]\tTest-mae:4.99769\n",
      "[121]\tTest-mae:4.98425\n",
      "[122]\tTest-mae:4.97175\n",
      "[123]\tTest-mae:4.95977\n",
      "[124]\tTest-mae:4.94747\n",
      "[125]\tTest-mae:4.93526\n",
      "[126]\tTest-mae:4.92187\n",
      "[127]\tTest-mae:4.90915\n",
      "[128]\tTest-mae:4.89631\n",
      "[129]\tTest-mae:4.88397\n",
      "[130]\tTest-mae:4.87183\n",
      "[131]\tTest-mae:4.85974\n",
      "[132]\tTest-mae:4.848\n",
      "[133]\tTest-mae:4.83554\n",
      "[134]\tTest-mae:4.82367\n",
      "[135]\tTest-mae:4.81243\n",
      "[136]\tTest-mae:4.80165\n",
      "[137]\tTest-mae:4.78984\n",
      "[138]\tTest-mae:4.77916\n",
      "[139]\tTest-mae:4.76692\n",
      "[140]\tTest-mae:4.75601\n",
      "[141]\tTest-mae:4.74517\n",
      "[142]\tTest-mae:4.73455\n",
      "[143]\tTest-mae:4.72343\n",
      "[144]\tTest-mae:4.71349\n",
      "[145]\tTest-mae:4.70298\n",
      "[146]\tTest-mae:4.69296\n",
      "[147]\tTest-mae:4.68212\n",
      "[148]\tTest-mae:4.67229\n",
      "[149]\tTest-mae:4.66224\n",
      "[150]\tTest-mae:4.65076\n",
      "[151]\tTest-mae:4.64063\n",
      "[152]\tTest-mae:4.62997\n",
      "[153]\tTest-mae:4.62049\n",
      "[154]\tTest-mae:4.61192\n",
      "[155]\tTest-mae:4.60256\n",
      "[156]\tTest-mae:4.59298\n",
      "[157]\tTest-mae:4.58371\n",
      "[158]\tTest-mae:4.57513\n",
      "[159]\tTest-mae:4.56692\n",
      "[160]\tTest-mae:4.55835\n",
      "[161]\tTest-mae:4.54899\n",
      "[162]\tTest-mae:4.54054\n",
      "[163]\tTest-mae:4.53217\n",
      "[164]\tTest-mae:4.52375\n",
      "[165]\tTest-mae:4.51539\n",
      "[166]\tTest-mae:4.50839\n",
      "[167]\tTest-mae:4.5005\n",
      "[168]\tTest-mae:4.4916\n",
      "[169]\tTest-mae:4.48431\n",
      "[170]\tTest-mae:4.47579\n",
      "[171]\tTest-mae:4.46659\n",
      "[172]\tTest-mae:4.46\n",
      "[173]\tTest-mae:4.45256\n",
      "[174]\tTest-mae:4.44472\n",
      "[175]\tTest-mae:4.43802\n",
      "[176]\tTest-mae:4.42998\n",
      "[177]\tTest-mae:4.42302\n",
      "[178]\tTest-mae:4.41674\n",
      "[179]\tTest-mae:4.41042\n",
      "[180]\tTest-mae:4.40363\n",
      "[181]\tTest-mae:4.39638\n",
      "[182]\tTest-mae:4.38873\n",
      "[183]\tTest-mae:4.38113\n",
      "[184]\tTest-mae:4.37418\n",
      "[185]\tTest-mae:4.36776\n",
      "[186]\tTest-mae:4.36112\n",
      "[187]\tTest-mae:4.354\n",
      "[188]\tTest-mae:4.34807\n",
      "[189]\tTest-mae:4.34194\n",
      "[190]\tTest-mae:4.33654\n",
      "[191]\tTest-mae:4.3313\n",
      "[192]\tTest-mae:4.32513\n",
      "[193]\tTest-mae:4.31796\n",
      "[194]\tTest-mae:4.31149\n",
      "[195]\tTest-mae:4.30487\n",
      "[196]\tTest-mae:4.29894\n",
      "[197]\tTest-mae:4.29238\n",
      "[198]\tTest-mae:4.2869\n",
      "[199]\tTest-mae:4.281\n",
      "[200]\tTest-mae:4.2761\n",
      "[201]\tTest-mae:4.26947\n",
      "[202]\tTest-mae:4.26426\n",
      "[203]\tTest-mae:4.25872\n",
      "[204]\tTest-mae:4.25272\n",
      "[205]\tTest-mae:4.24819\n",
      "[206]\tTest-mae:4.24217\n",
      "[207]\tTest-mae:4.23689\n",
      "[208]\tTest-mae:4.23118\n",
      "[209]\tTest-mae:4.22538\n",
      "[210]\tTest-mae:4.21968\n",
      "[211]\tTest-mae:4.21388\n",
      "[212]\tTest-mae:4.20882\n",
      "[213]\tTest-mae:4.20439\n",
      "[214]\tTest-mae:4.19826\n",
      "[215]\tTest-mae:4.19253\n",
      "[216]\tTest-mae:4.18746\n",
      "[217]\tTest-mae:4.18353\n",
      "[218]\tTest-mae:4.17842\n",
      "[219]\tTest-mae:4.17419\n",
      "[220]\tTest-mae:4.17019\n",
      "[221]\tTest-mae:4.16503\n",
      "[222]\tTest-mae:4.16142\n",
      "[223]\tTest-mae:4.15783\n",
      "[224]\tTest-mae:4.15423\n",
      "[225]\tTest-mae:4.15051\n",
      "[226]\tTest-mae:4.14633\n",
      "[227]\tTest-mae:4.14287\n",
      "[228]\tTest-mae:4.1388\n",
      "[229]\tTest-mae:4.13547\n",
      "[230]\tTest-mae:4.13098\n",
      "[231]\tTest-mae:4.12646\n",
      "[232]\tTest-mae:4.12298\n",
      "[233]\tTest-mae:4.11917\n",
      "[234]\tTest-mae:4.11574\n",
      "[235]\tTest-mae:4.11192\n",
      "[236]\tTest-mae:4.10792\n",
      "[237]\tTest-mae:4.10409\n",
      "[238]\tTest-mae:4.10073\n",
      "[239]\tTest-mae:4.09702\n",
      "[240]\tTest-mae:4.09468\n",
      "[241]\tTest-mae:4.0914\n",
      "[242]\tTest-mae:4.08785\n",
      "[243]\tTest-mae:4.08526\n",
      "[244]\tTest-mae:4.08258\n",
      "[245]\tTest-mae:4.07901\n",
      "[246]\tTest-mae:4.07517\n",
      "[247]\tTest-mae:4.07165\n",
      "[248]\tTest-mae:4.06899\n",
      "[249]\tTest-mae:4.06543\n",
      "[250]\tTest-mae:4.06239\n",
      "[251]\tTest-mae:4.05897\n",
      "[252]\tTest-mae:4.05715\n",
      "[253]\tTest-mae:4.05469\n",
      "[254]\tTest-mae:4.05205\n",
      "[255]\tTest-mae:4.04979\n",
      "[256]\tTest-mae:4.04701\n",
      "[257]\tTest-mae:4.04568\n",
      "[258]\tTest-mae:4.04272\n",
      "[259]\tTest-mae:4.03978\n",
      "[260]\tTest-mae:4.03753\n",
      "[261]\tTest-mae:4.03535\n",
      "[262]\tTest-mae:4.03345\n",
      "[263]\tTest-mae:4.03187\n",
      "[264]\tTest-mae:4.02921\n",
      "[265]\tTest-mae:4.0279\n",
      "[266]\tTest-mae:4.02551\n",
      "[267]\tTest-mae:4.02321\n",
      "[268]\tTest-mae:4.02128\n",
      "[269]\tTest-mae:4.01959\n",
      "[270]\tTest-mae:4.0175\n",
      "[271]\tTest-mae:4.01602\n",
      "[272]\tTest-mae:4.01427\n",
      "[273]\tTest-mae:4.01312\n",
      "[274]\tTest-mae:4.01054\n",
      "[275]\tTest-mae:4.00733\n",
      "[276]\tTest-mae:4.00547\n",
      "[277]\tTest-mae:4.00301\n",
      "[278]\tTest-mae:4.00081\n",
      "[279]\tTest-mae:3.99836\n",
      "[280]\tTest-mae:3.99756\n",
      "[281]\tTest-mae:3.99427\n",
      "[282]\tTest-mae:3.99253\n",
      "[283]\tTest-mae:3.99068\n",
      "[284]\tTest-mae:3.989\n",
      "[285]\tTest-mae:3.98752\n",
      "[286]\tTest-mae:3.98552\n",
      "[287]\tTest-mae:3.98285\n",
      "[288]\tTest-mae:3.98144\n",
      "[289]\tTest-mae:3.97947\n",
      "[290]\tTest-mae:3.97759\n",
      "[291]\tTest-mae:3.97652\n",
      "[292]\tTest-mae:3.9755\n",
      "[293]\tTest-mae:3.97425\n",
      "[294]\tTest-mae:3.97288\n",
      "[295]\tTest-mae:3.97076\n",
      "[296]\tTest-mae:3.96984\n",
      "[297]\tTest-mae:3.96763\n",
      "[298]\tTest-mae:3.96613\n",
      "[299]\tTest-mae:3.96466\n",
      "[300]\tTest-mae:3.9631\n",
      "[301]\tTest-mae:3.96233\n",
      "[302]\tTest-mae:3.96107\n",
      "[303]\tTest-mae:3.96032\n",
      "[304]\tTest-mae:3.95991\n",
      "[305]\tTest-mae:3.95934\n",
      "[306]\tTest-mae:3.95683\n",
      "[307]\tTest-mae:3.95578\n",
      "[308]\tTest-mae:3.95439\n",
      "[309]\tTest-mae:3.95412\n",
      "[310]\tTest-mae:3.95183\n",
      "[311]\tTest-mae:3.94992\n",
      "[312]\tTest-mae:3.9488\n",
      "[313]\tTest-mae:3.94882\n",
      "[314]\tTest-mae:3.94686\n",
      "[315]\tTest-mae:3.94661\n",
      "[316]\tTest-mae:3.94563\n",
      "[317]\tTest-mae:3.94513\n",
      "[318]\tTest-mae:3.94461\n",
      "[319]\tTest-mae:3.94389\n",
      "[320]\tTest-mae:3.94353\n",
      "[321]\tTest-mae:3.94308\n",
      "[322]\tTest-mae:3.94252\n",
      "[323]\tTest-mae:3.94201\n",
      "[324]\tTest-mae:3.94108\n",
      "[325]\tTest-mae:3.94015\n",
      "[326]\tTest-mae:3.94059\n",
      "[327]\tTest-mae:3.93971\n",
      "[328]\tTest-mae:3.93895\n",
      "[329]\tTest-mae:3.9392\n",
      "[330]\tTest-mae:3.93876\n",
      "[331]\tTest-mae:3.93838\n",
      "[332]\tTest-mae:3.93777\n",
      "[333]\tTest-mae:3.93723\n",
      "[334]\tTest-mae:3.9359\n",
      "[335]\tTest-mae:3.93517\n",
      "[336]\tTest-mae:3.93441\n",
      "[337]\tTest-mae:3.93455\n",
      "[338]\tTest-mae:3.93446\n",
      "[339]\tTest-mae:3.93285\n",
      "[340]\tTest-mae:3.93194\n",
      "[341]\tTest-mae:3.93093\n",
      "[342]\tTest-mae:3.93058\n",
      "[343]\tTest-mae:3.93013\n",
      "[344]\tTest-mae:3.93022\n",
      "[345]\tTest-mae:3.93027\n",
      "[346]\tTest-mae:3.9312\n",
      "[347]\tTest-mae:3.93057\n",
      "[348]\tTest-mae:3.93043\n",
      "[349]\tTest-mae:3.93063\n",
      "[350]\tTest-mae:3.9294\n",
      "[351]\tTest-mae:3.92977\n",
      "[352]\tTest-mae:3.92841\n",
      "[353]\tTest-mae:3.92852\n",
      "[354]\tTest-mae:3.92847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355]\tTest-mae:3.92952\n",
      "[356]\tTest-mae:3.92947\n",
      "[357]\tTest-mae:3.92995\n",
      "[358]\tTest-mae:3.92974\n",
      "[359]\tTest-mae:3.92965\n",
      "[360]\tTest-mae:3.92935\n",
      "[361]\tTest-mae:3.92811\n",
      "[362]\tTest-mae:3.92744\n",
      "[363]\tTest-mae:3.92652\n",
      "[364]\tTest-mae:3.92786\n",
      "[365]\tTest-mae:3.9285\n",
      "[366]\tTest-mae:3.92846\n",
      "[367]\tTest-mae:3.92828\n",
      "[368]\tTest-mae:3.9279\n",
      "[369]\tTest-mae:3.92785\n",
      "[370]\tTest-mae:3.92766\n",
      "[371]\tTest-mae:3.92686\n",
      "[372]\tTest-mae:3.92665\n",
      "[373]\tTest-mae:3.92671\n",
      "Stopping. Best iteration:\n",
      "[363]\tTest-mae:3.92652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tTest-mae:7.72516\n",
      "[1]\tTest-mae:7.68929\n",
      "[2]\tTest-mae:7.65557\n",
      "[3]\tTest-mae:7.61957\n",
      "[4]\tTest-mae:7.5839\n",
      "[5]\tTest-mae:7.54856\n",
      "[6]\tTest-mae:7.51315\n",
      "[7]\tTest-mae:7.48022\n",
      "[8]\tTest-mae:7.44596\n",
      "[9]\tTest-mae:7.41321\n",
      "[10]\tTest-mae:7.37869\n",
      "[11]\tTest-mae:7.34604\n",
      "[12]\tTest-mae:7.31248\n",
      "[13]\tTest-mae:7.2791\n",
      "[14]\tTest-mae:7.24591\n",
      "[15]\tTest-mae:7.21429\n",
      "[16]\tTest-mae:7.18185\n",
      "[17]\tTest-mae:7.14933\n",
      "[18]\tTest-mae:7.11726\n",
      "[19]\tTest-mae:7.08604\n",
      "[20]\tTest-mae:7.05399\n",
      "[21]\tTest-mae:7.02381\n",
      "[22]\tTest-mae:6.99194\n",
      "[23]\tTest-mae:6.96059\n",
      "[24]\tTest-mae:6.92885\n",
      "[25]\tTest-mae:6.89883\n",
      "[26]\tTest-mae:6.86869\n",
      "[27]\tTest-mae:6.84059\n",
      "[28]\tTest-mae:6.81215\n",
      "[29]\tTest-mae:6.78097\n",
      "[30]\tTest-mae:6.75338\n",
      "[31]\tTest-mae:6.72323\n",
      "[32]\tTest-mae:6.69574\n",
      "[33]\tTest-mae:6.66586\n",
      "[34]\tTest-mae:6.63746\n",
      "[35]\tTest-mae:6.60968\n",
      "[36]\tTest-mae:6.58174\n",
      "[37]\tTest-mae:6.55553\n",
      "[38]\tTest-mae:6.52827\n",
      "[39]\tTest-mae:6.50107\n",
      "[40]\tTest-mae:6.47334\n",
      "[41]\tTest-mae:6.44917\n",
      "[42]\tTest-mae:6.42616\n",
      "[43]\tTest-mae:6.39965\n",
      "[44]\tTest-mae:6.37425\n",
      "[45]\tTest-mae:6.34918\n",
      "[46]\tTest-mae:6.32219\n",
      "[47]\tTest-mae:6.29722\n",
      "[48]\tTest-mae:6.27235\n",
      "[49]\tTest-mae:6.24652\n",
      "[50]\tTest-mae:6.22324\n",
      "[51]\tTest-mae:6.19966\n",
      "[52]\tTest-mae:6.17828\n",
      "[53]\tTest-mae:6.15846\n",
      "[54]\tTest-mae:6.13553\n",
      "[55]\tTest-mae:6.11277\n",
      "[56]\tTest-mae:6.09179\n",
      "[57]\tTest-mae:6.06978\n",
      "[58]\tTest-mae:6.04734\n",
      "[59]\tTest-mae:6.02683\n",
      "[60]\tTest-mae:6.00448\n",
      "[61]\tTest-mae:5.98486\n",
      "[62]\tTest-mae:5.96288\n",
      "[63]\tTest-mae:5.9428\n",
      "[64]\tTest-mae:5.92207\n",
      "[65]\tTest-mae:5.90204\n",
      "[66]\tTest-mae:5.88134\n",
      "[67]\tTest-mae:5.86229\n",
      "[68]\tTest-mae:5.84278\n",
      "[69]\tTest-mae:5.82292\n",
      "[70]\tTest-mae:5.80246\n",
      "[71]\tTest-mae:5.78308\n",
      "[72]\tTest-mae:5.76214\n",
      "[73]\tTest-mae:5.74275\n",
      "[74]\tTest-mae:5.72307\n",
      "[75]\tTest-mae:5.70526\n",
      "[76]\tTest-mae:5.68738\n",
      "[77]\tTest-mae:5.66891\n",
      "[78]\tTest-mae:5.64965\n",
      "[79]\tTest-mae:5.63011\n",
      "[80]\tTest-mae:5.61244\n",
      "[81]\tTest-mae:5.59495\n",
      "[82]\tTest-mae:5.57802\n",
      "[83]\tTest-mae:5.55997\n",
      "[84]\tTest-mae:5.54248\n",
      "[85]\tTest-mae:5.52402\n",
      "[86]\tTest-mae:5.50544\n",
      "[87]\tTest-mae:5.48793\n",
      "[88]\tTest-mae:5.47144\n",
      "[89]\tTest-mae:5.45407\n",
      "[90]\tTest-mae:5.43767\n",
      "[91]\tTest-mae:5.42289\n",
      "[92]\tTest-mae:5.40552\n",
      "[93]\tTest-mae:5.38992\n",
      "[94]\tTest-mae:5.37418\n",
      "[95]\tTest-mae:5.35804\n",
      "[96]\tTest-mae:5.34324\n",
      "[97]\tTest-mae:5.32699\n",
      "[98]\tTest-mae:5.31168\n",
      "[99]\tTest-mae:5.29593\n",
      "[100]\tTest-mae:5.28041\n",
      "[101]\tTest-mae:5.26556\n",
      "[102]\tTest-mae:5.25007\n",
      "[103]\tTest-mae:5.23694\n",
      "[104]\tTest-mae:5.22247\n",
      "[105]\tTest-mae:5.20872\n",
      "[106]\tTest-mae:5.19456\n",
      "[107]\tTest-mae:5.17902\n",
      "[108]\tTest-mae:5.16427\n",
      "[109]\tTest-mae:5.14971\n",
      "[110]\tTest-mae:5.13717\n",
      "[111]\tTest-mae:5.12298\n",
      "[112]\tTest-mae:5.1099\n",
      "[113]\tTest-mae:5.09539\n",
      "[114]\tTest-mae:5.08056\n",
      "[115]\tTest-mae:5.0666\n",
      "[116]\tTest-mae:5.05295\n",
      "[117]\tTest-mae:5.03837\n",
      "[118]\tTest-mae:5.02372\n",
      "[119]\tTest-mae:5.00985\n",
      "[120]\tTest-mae:4.99769\n",
      "[121]\tTest-mae:4.98425\n",
      "[122]\tTest-mae:4.97175\n",
      "[123]\tTest-mae:4.95977\n",
      "[124]\tTest-mae:4.94747\n",
      "[125]\tTest-mae:4.93526\n",
      "[126]\tTest-mae:4.92187\n",
      "[127]\tTest-mae:4.90915\n",
      "[128]\tTest-mae:4.89631\n",
      "[129]\tTest-mae:4.88397\n",
      "[130]\tTest-mae:4.87183\n",
      "[131]\tTest-mae:4.85974\n",
      "[132]\tTest-mae:4.848\n",
      "[133]\tTest-mae:4.83554\n",
      "[134]\tTest-mae:4.82367\n",
      "[135]\tTest-mae:4.81243\n",
      "[136]\tTest-mae:4.80165\n",
      "[137]\tTest-mae:4.78984\n",
      "[138]\tTest-mae:4.77916\n",
      "[139]\tTest-mae:4.76692\n",
      "[140]\tTest-mae:4.75601\n",
      "[141]\tTest-mae:4.74517\n",
      "[142]\tTest-mae:4.73455\n",
      "[143]\tTest-mae:4.72343\n",
      "[144]\tTest-mae:4.71349\n",
      "[145]\tTest-mae:4.70298\n",
      "[146]\tTest-mae:4.69296\n",
      "[147]\tTest-mae:4.68212\n",
      "[148]\tTest-mae:4.67229\n",
      "[149]\tTest-mae:4.66224\n",
      "[150]\tTest-mae:4.65076\n",
      "[151]\tTest-mae:4.64063\n",
      "[152]\tTest-mae:4.62997\n",
      "[153]\tTest-mae:4.62049\n",
      "[154]\tTest-mae:4.61192\n",
      "[155]\tTest-mae:4.60256\n",
      "[156]\tTest-mae:4.59298\n",
      "[157]\tTest-mae:4.58371\n",
      "[158]\tTest-mae:4.57513\n",
      "[159]\tTest-mae:4.56692\n",
      "[160]\tTest-mae:4.55835\n",
      "[161]\tTest-mae:4.54899\n",
      "[162]\tTest-mae:4.54054\n",
      "[163]\tTest-mae:4.53217\n",
      "[164]\tTest-mae:4.52375\n",
      "[165]\tTest-mae:4.51539\n",
      "[166]\tTest-mae:4.50839\n",
      "[167]\tTest-mae:4.5005\n",
      "[168]\tTest-mae:4.4916\n",
      "[169]\tTest-mae:4.48431\n",
      "[170]\tTest-mae:4.47579\n",
      "[171]\tTest-mae:4.46659\n",
      "[172]\tTest-mae:4.46\n",
      "[173]\tTest-mae:4.45256\n",
      "[174]\tTest-mae:4.44472\n",
      "[175]\tTest-mae:4.43802\n",
      "[176]\tTest-mae:4.42998\n",
      "[177]\tTest-mae:4.42302\n",
      "[178]\tTest-mae:4.41674\n",
      "[179]\tTest-mae:4.41042\n",
      "[180]\tTest-mae:4.40363\n",
      "[181]\tTest-mae:4.39638\n",
      "[182]\tTest-mae:4.38873\n",
      "[183]\tTest-mae:4.38113\n",
      "[184]\tTest-mae:4.37418\n",
      "[185]\tTest-mae:4.36776\n",
      "[186]\tTest-mae:4.36112\n",
      "[187]\tTest-mae:4.354\n",
      "[188]\tTest-mae:4.34807\n",
      "[189]\tTest-mae:4.34194\n",
      "[190]\tTest-mae:4.33654\n",
      "[191]\tTest-mae:4.3313\n",
      "[192]\tTest-mae:4.32513\n",
      "[193]\tTest-mae:4.31796\n",
      "[194]\tTest-mae:4.31149\n",
      "[195]\tTest-mae:4.30487\n",
      "[196]\tTest-mae:4.29894\n",
      "[197]\tTest-mae:4.29238\n",
      "[198]\tTest-mae:4.2869\n",
      "[199]\tTest-mae:4.281\n",
      "[200]\tTest-mae:4.2761\n",
      "[201]\tTest-mae:4.26947\n",
      "[202]\tTest-mae:4.26426\n",
      "[203]\tTest-mae:4.25872\n",
      "[204]\tTest-mae:4.25272\n",
      "[205]\tTest-mae:4.24819\n",
      "[206]\tTest-mae:4.24217\n",
      "[207]\tTest-mae:4.23689\n",
      "[208]\tTest-mae:4.23118\n",
      "[209]\tTest-mae:4.22538\n",
      "[210]\tTest-mae:4.21968\n",
      "[211]\tTest-mae:4.21388\n",
      "[212]\tTest-mae:4.20882\n",
      "[213]\tTest-mae:4.20439\n",
      "[214]\tTest-mae:4.19826\n",
      "[215]\tTest-mae:4.19253\n",
      "[216]\tTest-mae:4.18746\n",
      "[217]\tTest-mae:4.18353\n",
      "[218]\tTest-mae:4.17842\n",
      "[219]\tTest-mae:4.17419\n",
      "[220]\tTest-mae:4.17019\n",
      "[221]\tTest-mae:4.16503\n",
      "[222]\tTest-mae:4.16142\n",
      "[223]\tTest-mae:4.15783\n",
      "[224]\tTest-mae:4.15423\n",
      "[225]\tTest-mae:4.15051\n",
      "[226]\tTest-mae:4.14633\n",
      "[227]\tTest-mae:4.14287\n",
      "[228]\tTest-mae:4.1388\n",
      "[229]\tTest-mae:4.13547\n",
      "[230]\tTest-mae:4.13098\n",
      "[231]\tTest-mae:4.12646\n",
      "[232]\tTest-mae:4.12298\n",
      "[233]\tTest-mae:4.11917\n",
      "[234]\tTest-mae:4.11574\n",
      "[235]\tTest-mae:4.11192\n",
      "[236]\tTest-mae:4.10792\n",
      "[237]\tTest-mae:4.10409\n",
      "[238]\tTest-mae:4.10073\n",
      "[239]\tTest-mae:4.09702\n",
      "[240]\tTest-mae:4.09468\n",
      "[241]\tTest-mae:4.0914\n",
      "[242]\tTest-mae:4.08785\n",
      "[243]\tTest-mae:4.08526\n",
      "[244]\tTest-mae:4.08258\n",
      "[245]\tTest-mae:4.07901\n",
      "[246]\tTest-mae:4.07517\n",
      "[247]\tTest-mae:4.07165\n",
      "[248]\tTest-mae:4.06899\n",
      "[249]\tTest-mae:4.06543\n",
      "[250]\tTest-mae:4.06239\n",
      "[251]\tTest-mae:4.05897\n",
      "[252]\tTest-mae:4.05715\n",
      "[253]\tTest-mae:4.05469\n",
      "[254]\tTest-mae:4.05205\n",
      "[255]\tTest-mae:4.04979\n",
      "[256]\tTest-mae:4.04701\n",
      "[257]\tTest-mae:4.04568\n",
      "[258]\tTest-mae:4.04272\n",
      "[259]\tTest-mae:4.03978\n",
      "[260]\tTest-mae:4.03753\n",
      "[261]\tTest-mae:4.03535\n",
      "[262]\tTest-mae:4.03345\n",
      "[263]\tTest-mae:4.03187\n",
      "[264]\tTest-mae:4.02921\n",
      "[265]\tTest-mae:4.0279\n",
      "[266]\tTest-mae:4.02551\n",
      "[267]\tTest-mae:4.02321\n",
      "[268]\tTest-mae:4.02128\n",
      "[269]\tTest-mae:4.01959\n",
      "[270]\tTest-mae:4.0175\n",
      "[271]\tTest-mae:4.01602\n",
      "[272]\tTest-mae:4.01427\n",
      "[273]\tTest-mae:4.01312\n",
      "[274]\tTest-mae:4.01054\n",
      "[275]\tTest-mae:4.00733\n",
      "[276]\tTest-mae:4.00547\n",
      "[277]\tTest-mae:4.00301\n",
      "[278]\tTest-mae:4.00081\n",
      "[279]\tTest-mae:3.99836\n",
      "[280]\tTest-mae:3.99756\n",
      "[281]\tTest-mae:3.99427\n",
      "[282]\tTest-mae:3.99253\n",
      "[283]\tTest-mae:3.99068\n",
      "[284]\tTest-mae:3.989\n",
      "[285]\tTest-mae:3.98752\n",
      "[286]\tTest-mae:3.98552\n",
      "[287]\tTest-mae:3.98285\n",
      "[288]\tTest-mae:3.98144\n",
      "[289]\tTest-mae:3.97947\n",
      "[290]\tTest-mae:3.97759\n",
      "[291]\tTest-mae:3.97652\n",
      "[292]\tTest-mae:3.9755\n",
      "[293]\tTest-mae:3.97425\n",
      "[294]\tTest-mae:3.97288\n",
      "[295]\tTest-mae:3.97076\n",
      "[296]\tTest-mae:3.96984\n",
      "[297]\tTest-mae:3.96763\n",
      "[298]\tTest-mae:3.96613\n",
      "[299]\tTest-mae:3.96466\n",
      "[300]\tTest-mae:3.9631\n",
      "[301]\tTest-mae:3.96233\n",
      "[302]\tTest-mae:3.96107\n",
      "[303]\tTest-mae:3.96032\n",
      "[304]\tTest-mae:3.95991\n",
      "[305]\tTest-mae:3.95934\n",
      "[306]\tTest-mae:3.95683\n",
      "[307]\tTest-mae:3.95578\n",
      "[308]\tTest-mae:3.95439\n",
      "[309]\tTest-mae:3.95412\n",
      "[310]\tTest-mae:3.95183\n",
      "[311]\tTest-mae:3.94992\n",
      "[312]\tTest-mae:3.9488\n",
      "[313]\tTest-mae:3.94882\n",
      "[314]\tTest-mae:3.94686\n",
      "[315]\tTest-mae:3.94661\n",
      "[316]\tTest-mae:3.94563\n",
      "[317]\tTest-mae:3.94513\n",
      "[318]\tTest-mae:3.94461\n",
      "[319]\tTest-mae:3.94389\n",
      "[320]\tTest-mae:3.94353\n",
      "[321]\tTest-mae:3.94308\n",
      "[322]\tTest-mae:3.94252\n",
      "[323]\tTest-mae:3.94201\n",
      "[324]\tTest-mae:3.94108\n",
      "[325]\tTest-mae:3.94015\n",
      "[326]\tTest-mae:3.94059\n",
      "[327]\tTest-mae:3.93971\n",
      "[328]\tTest-mae:3.93895\n",
      "[329]\tTest-mae:3.9392\n",
      "[330]\tTest-mae:3.93876\n",
      "[331]\tTest-mae:3.93838\n",
      "[332]\tTest-mae:3.93777\n",
      "[333]\tTest-mae:3.93723\n",
      "[334]\tTest-mae:3.9359\n",
      "[335]\tTest-mae:3.93517\n",
      "[336]\tTest-mae:3.93441\n",
      "[337]\tTest-mae:3.93455\n",
      "[338]\tTest-mae:3.93446\n",
      "[339]\tTest-mae:3.93285\n",
      "[340]\tTest-mae:3.93194\n",
      "[341]\tTest-mae:3.93093\n",
      "[342]\tTest-mae:3.93058\n",
      "[343]\tTest-mae:3.93013\n",
      "[344]\tTest-mae:3.93022\n",
      "[345]\tTest-mae:3.93027\n",
      "[346]\tTest-mae:3.9312\n",
      "[347]\tTest-mae:3.93057\n",
      "[348]\tTest-mae:3.93043\n",
      "[349]\tTest-mae:3.93063\n",
      "[350]\tTest-mae:3.9294\n",
      "[351]\tTest-mae:3.92977\n",
      "[352]\tTest-mae:3.92841\n",
      "[353]\tTest-mae:3.92852\n",
      "[354]\tTest-mae:3.92847\n",
      "[355]\tTest-mae:3.92952\n",
      "[356]\tTest-mae:3.92947\n",
      "[357]\tTest-mae:3.92995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[358]\tTest-mae:3.92974\n",
      "[359]\tTest-mae:3.92965\n",
      "[360]\tTest-mae:3.92935\n",
      "[361]\tTest-mae:3.92811\n",
      "[362]\tTest-mae:3.92744\n",
      "[363]\tTest-mae:3.92652\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = model.best_iteration + 1\n",
    "\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbest_model.save_model(\"my_model.model\")\\n\\nloaded_model = xgb.Booster()\\nloaded_model.load_model(\"my_model.model\")\\n# And use it for predictions.\\nloaded_model.predict(dtest)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "best_model.save_model(\"my_model.model\")\n",
    "\n",
    "loaded_model = xgb.Booster()\n",
    "loaded_model.load_model(\"my_model.model\")\n",
    "# And use it for predictions.\n",
    "loaded_model.predict(dtest)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- We should obtain the **same** score as promised in the last round of training, let’s check!\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9265177537378957"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(best_model.predict(dtest), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
