{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What? Building non-correlated ensembles\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This datasets can be ued to predicts Uber and Lyft cab prices.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(model):\n",
    "    # Obtain scores of cross-validation using 5 splits\n",
    "    scores = cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "    # Return mean score\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9771619313771154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5357397919577706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster='gblinear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736376339077782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster='dart', one_drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666356155876418"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(RandomForestClassifier(random_state=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525694767893184"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(LogisticRegression(max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9771464058376027"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(n_estimators=800, max_depth=4, colsample_bylevel=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Most models perform respectably, with the XGBoost classifier obtaining the highest score. The gblinear base learner \n",
    "did not perform particularly well, however, so we will not use it going forward.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The purpose of this section is not to select all models for the ensemble, but rather to select the \n",
    "non-correlated models. â€œCorrelation is a statistical measure between -1 and 1 that indicates the \n",
    "strength of the linear relationship between two sets of points. A correlation of 1 is a perfectly \n",
    "straight line, while a correlation of 0 shows no linear relationship whatsoever.\n",
    "\n",
    "A high correlation between machine learning models is undesirable in an ensemble. But why? Consider the\n",
    "case of two classifiers with 1,000 predictions each. If these classifiers all make the same predictions, \n",
    "no new information is gained from the second classifier, making it superfluous. Using a majority rules \n",
    "implementation, a prediction is only wrong if the majority of classifiers get it wrong. It's desirable, \n",
    "therefore, to have a diversity of models that score well but give different predictions. If most models\n",
    "give the same predictions, the correlation is high, and there is little value in adding the new model to\n",
    "the ensemble. Finding differences in predictions where a strong model may be wrong gives the ensemble the\n",
    "chance to produce better results. Predictions will be different when the models are non-correlated.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = accuracy_score(y_pred, y_test)\n",
    "    print(score)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To compute correlations between machine learning models, we first need data points to compare. The different \n",
    "data points that machine learning models produce are their predictions. After obtaining predictions, we \n",
    "concatenate them into a DataFrame, and then apply the .corr method to obtain all correlations at once.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "y_pred_gbtree = y_pred(XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "y_pred_dart = y_pred(XGBClassifier(booster='dart', one_drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "y_pred_forest = y_pred(RandomForestClassifier(random_state=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "y_pred_logistic = y_pred(LogisticRegression(max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = y_pred(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(data= np.c_[y_pred_gbtree, y_pred_dart, y_pred_forest, y_pred_logistic, y_pred_xgb], \n",
    "                  columns=['gbtree', 'dart', 'forest', 'logistic', 'xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbtree</th>\n",
       "      <th>dart</th>\n",
       "      <th>forest</th>\n",
       "      <th>logistic</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbtree</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>0.884584</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.971146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart</th>\n",
       "      <td>0.971146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913438</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.971146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest</th>\n",
       "      <td>0.884584</td>\n",
       "      <td>0.913438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943308</td>\n",
       "      <td>0.913438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic</th>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.943308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.971146</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>0.913438</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
""      ],
      "text/plain": [
       "            gbtree      dart    forest  logistic       xgb\n",
       "gbtree    1.000000  0.971146  0.884584  0.914111  0.971146\n",
       "dart      0.971146  1.000000  0.913438  0.914111  0.971146\n",
       "forest    0.884584  0.913438  1.000000  0.943308  0.913438\n",
       "logistic  0.914111  0.914111  0.943308  1.000000  0.914111\n",
       "xgb       0.971146  0.971146  0.913438  0.914111  1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There is no clear cut-off to obtain a non-correlated threshold. It ultimately depends on the values of correlation \n",
    "and the number of models to choose from. For this example, we could pick the next two least correlated models with\n",
    "our best model, xgb, which are the random forest and logistic regression. Now we\n",
    "will combine them into a single ensemble using the VotingClassifier ensemble, introduced next.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9771619313771154\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "logistic_model = LogisticRegression(max_iter=10000)\n",
    "xgb_model = XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1)\n",
    "rf_model = RandomForestClassifier(random_state=2)\n",
    "\n",
    "estimators.append(('logistic', logistic_model))\n",
    "estimators.append(('xgb', xgb_model))\n",
    "estimators.append(('rf', rf_model))\n",
    "\n",
    "ensemble = VotingClassifier(estimators)\n",
    "scores = cross_val_score(ensemble, X, y, cv=kfold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is important to note that the stacking classifier builds a model on the base models(or estimators) hereby \n",
    "increasing fit for the dataset. This also increases the tendency of the stacking classifier to overfit especially\n",
    "in the case of multi-layer stacking.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9789318428815401\n"
     ]
    }
   ],
   "source": [
    "base_models = []\n",
    "base_models.append(('lr', LogisticRegression()))\n",
    "base_models.append(('xgb', XGBClassifier()))\n",
    "base_models.append(('rf', RandomForestClassifier(random_state=2)))\n",
    "\n",
    "# Define meta learner model. This is something is not done in the voting classifier above\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# define the stacking ensemble\n",
    "clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "scores = cross_val_score(clf, X, y, cv=kfold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Corey Wade. â€œHands-On Gradient Boosting with XGBoost and scikit-learn\n",
    "- https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
